# Databricks Data Engineering Project

A production-ready, modular Databricks project with reusable code library, configuration-driven notebooks, and Databricks Asset Bundles (DAB) deployment across environments.

## ğŸ“ Project Structure

```
databricks-project/
â”œâ”€â”€ databricks.yaml                 # DAB main configuration
â”œâ”€â”€ resources/                      # DAB resource definitions
â”‚   â”œâ”€â”€ jobs.yml                    # Job definitions with placeholders
â”‚   â””â”€â”€ clusters.yml                # Cluster configurations
â”œâ”€â”€ src/
â”‚   â””â”€â”€ datalib/                    # Python library (wheel)
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ core/                   # Core processing logic
â”‚       â”œâ”€â”€ transformations/        # Data transformations
â”‚       â”œâ”€â”€ utils/                  # Utilities
â”‚       â””â”€â”€ io/                     # I/O operations
â”œâ”€â”€ jobs/                           # Job JSON configurations
â”‚   â”œâ”€â”€ bronze/
â”‚   â”œâ”€â”€ silver/
â”‚   â””â”€â”€ gold/
â”œâ”€â”€ notebooks/                      # Reusable notebooks
â”‚   â”œâ”€â”€ runner.py                   # Main reusable notebook
â”‚   â”œâ”€â”€ bronze/
â”‚   â”œâ”€â”€ silver/
â”‚   â””â”€â”€ gold/
â”œâ”€â”€ configs/                        # YAML configurations by business area
â”‚   â”œâ”€â”€ bronze/
â”‚   â”œâ”€â”€ silver/
â”‚   â””â”€â”€ gold/
â”œâ”€â”€ tests/                          # Unit tests
â”œâ”€â”€ scripts/                        # Build & deployment scripts
â”œâ”€â”€ setup.py                        # Package setup
â”œâ”€â”€ pyproject.toml                  # Modern Python packaging
â””â”€â”€ .github/workflows/              # CI/CD pipelines
```

## ğŸ—ï¸ Architecture

### Environments
| Environment | Code | Databricks Workspace | Purpose |
|-------------|------|---------------------|---------|
| Sandbox | dev | /Workspace/dev | Development & testing |
| Stage | qa | /Workspace/qa | QA & integration testing |
| Production | prd | /Workspace/prd | Production workloads |

### Versioning Strategy
- **Sandbox (dev)**: `{major}.{minor}.{patch}.dev{build}+{branch_name}`
  - Example: `1.0.0.dev42+feature-new-transform`
- **Stage (qa)**: `{major}.{minor+1}.0rc{build}`
  - Example: `1.1.0rc1`
- **Production (prd)**: Same wheel as QA, promoted
  - Example: `1.1.0` (release candidate promoted)

## ğŸš€ Quick Start

### Prerequisites
- Python 3.9+
- Databricks CLI v0.200+
- Access to Databricks workspace

### Setup

```bash
# Clone the repository
git clone <repository-url>
cd databricks-project

# Install development dependencies
pip install -e ".[dev]"

# Configure Databricks CLI
databricks configure --profile dev
databricks configure --profile qa
databricks configure --profile prd

# Validate DAB configuration
databricks bundle validate -t dev
```

### Build Wheel

```bash
# For development (includes branch name)
./scripts/build_wheel.sh dev feature-my-branch 1

# For QA (increments minor version)
./scripts/build_wheel.sh qa

# Wheel is automatically used in production from QA
```

### Deploy

```bash
# Deploy to sandbox
databricks bundle deploy -t dev

# Deploy to stage
databricks bundle deploy -t qa

# Deploy to production
databricks bundle deploy -t prd
```

## ğŸ“– Configuration Guide

### YAML Config Structure

Each business area (bronze/silver/gold) has YAML configs:

```yaml
# configs/bronze/customer_ingestion.yaml
pipeline:
  name: customer_ingestion
  layer: bronze
  
source:
  type: jdbc
  connection_string: "${JDBC_CONNECTION}"
  table: customers
  
target:
  catalog: main
  schema: bronze
  table: customers_raw
  
processing:
  mode: incremental
  watermark_column: updated_at
  partition_by: [ingestion_date]
```

### Job Configuration

Jobs reference YAML configs and use placeholders:

```json
{
  "job_cluster_key": "{{JOB_CLUSTER}}",
  "existing_cluster_id": "{{INTERACTIVE_CLUSTER_ID}}",
  "libraries": [
    {"whl": "{{WHEEL_PATH}}"}
  ]
}
```

## ğŸ”§ Development Workflow

1. **Create feature branch** from `main`
2. **Develop & test** locally
3. **Build wheel** for sandbox: `./scripts/build_wheel.sh dev <branch> <build>`
4. **Deploy to sandbox**: `databricks bundle deploy -t dev`
5. **Test in Databricks** sandbox workspace
6. **Create PR** to `main`
7. **After merge**, CI/CD builds QA wheel and deploys
8. **After QA approval**, promote to production

## ğŸ“Š Business Areas

### Bronze Layer
Raw data ingestion from source systems.

### Silver Layer
Cleansed, conformed data with business logic.

### Gold Layer
Aggregated, business-ready datasets.

## ğŸ§ª Testing

```bash
# Run unit tests
pytest tests/ -v

# Run with coverage
pytest tests/ --cov=datalib --cov-report=html
```

## ğŸ“š Additional Documentation

- [Configuration Reference](docs/configuration.md)
- [Deployment Guide](docs/deployment.md)
- [Development Guide](docs/development.md)
